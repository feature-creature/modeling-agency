> [T]he “knower” does not stand in a relation of absolute externality to the natural world being investigated—there is no such exterior observational point. […] “We” are not outside observers of the world. Nor are we simply located at particular places in the world; rather, we are part of the world in its ongoing intra-activity. [1]

# Introduction

Our situation in the world we inhabit has necessarily influenced the concepts we have formed to understand and act within it. In one sense this is a matter of perspective. We are beings of a particular size, on a particular planet at a particular time, equipped with the particular set of capacities history has left us with. But there is another aspect to this influence which has constrained and shaped our conceptual habitat in a more fundamental way: we are not passive observers but beings with interests, agents acting on and within the world. We do not just watch and wait for outcomes, we care about them and try to influence them. The categories with which we carve up the world cannot only be said to represent the world, they must also articulate the world in a way which proves useful in our dealings with it.

Utility and representational accuracy may overlap in specific instances, but there’s no reason to suppose they must do so in general. The significance of this point comes into view when we consider the way humans have historically attributed agency to nonhuman entities and events. To explain the success of a crop by appeal to the favour of the field spirits, or a natural disaster by the wrath of the Gods is nowadays dismissed as anthropomorphism, guilty of projecting human intentions and subjectivities onto a natural world where none exist. If these explanations have in the past been useful to us this owes more, so the dismissal goes, to the way we are than the way the world is. These explanations have now been superseded by modern theories which explain such phenomena in terms of physical processes unfolding deterministically in accordance with causal laws, explanations which nowhere make reference to agents with intentions or subjectivity.

The corrosion of agency in the natural world reaches its full expression when this logic is turned back upon the situated human. We are as much explained by deterministic (or quasi-deterministic) physical processes as the natural world we inhabit, and so our own internal machinations and external actions can also be explained in terms of unguided causal laws. 

Does this mean that human agency is also a fiction? If so, does this mean we should stop talking about each other as if we are actors with interests and concerns? Or is this kind of talk justified by its utility? If agency is not an objective property of individual humans may it not be, for example, some kind of valuable heuristic for navigating social interactions? That is to say, might not human agency be an inherently useful fiction? Even so, does the denial of reality embodied in the word ‘fiction’ not depend on overly representationalist criteria of the real, conceding reality only to categories identifying objective properties of entities? Would not those of a more pragmatist inclination argue that this view depends on an unsustainably naive realism, an assumption that the world is articulated into categories prior to our draping a conceptual net upon it (arguably a form of anthropomorphism in itself), and that a category’s intrinsic utility is sufficient to underwrite its reality regardless of whether it ‘corresponds to an objective property’ or not?  

Whatever the answers to these questions, what does seem clear is that turning the material-mechanistic rationale back on ourselves levels the playing field between humans and nonhumans. If determinism undermines agency in the nonhuman then it does so in the human also. If human agency can remain (in one way or another) in a deterministic world then there is no reason to suppose that nonhuman agency cannot. In both cases there remain significant questions about why it is more useful to describe some systems as agents than others, why genes and countries can helpfully be modelled as rational actors and studied with game theory when galaxies cannot, or why it still feels useful to describe and explain the behaviours of humans and octopuses with reference to intentions and purposes when this no longer feels appropriate for crops and volcanoes. 

# Why does it Matter?

Are there any general criteria for establishing the agenthood of a physical system? If so, are there any other nonhuman systems besides the ones so far mentioned that exhibit these features? If there are, why is it important to recognise these systems as agents in their own right? 

Software presents a vivid case study, one which will be taken up in what follows. On the one hand, their status as human artefacts make computational processes difficult for humans to regard as autonomous agents, even when they present many of the behaviours that we might find indicative of agency in, say, nonhuman animals. On the other hand, we live in a world where ever greater degrees of autonomy and responsibility are parcelled out to computational processes, processes whose influence is no longer confined to the digital domain but extends into the physical environment as systems are developed to use realtime data gathering to manage microclimates, crowd flows, air pollution, animal populations, and so on.

Ulrik Ekman writes of these ‘smart’ environments,  

> A supposedly ‘natural’ setting turns out to be nothing if not a highly artificial context or an information-intensive environment, and it appears attentively oriented towards us rather than being neutral or perfectly non-caring. […] Their directedness towards us appears to come not from distant otherness but rather from intimate sameness, in the sense that these processes and events often have the character of anticipating what we intend in consciousness and in embodied practices. Then, we are also, and not least, disturbed by vestiges of autonomy, artificial intelligence, and artificial life: Environmental spacing and temporalization harbors an unexpected form of intelligently organized and organizing complexity, different but perhaps not necessarily altogether foreign to the ways in which we exist, think, and comfort ourselves. [2]

Ekman here points to some of the features of these systems that seem to us uncannily agential. They possess a degree of autonomy in that they respond to novel circumstances without direct human control; they are sensitive to changes of external conditions; they exhibit complexity not only in their internal organisation, but in organising their external environment. These features combine to give us a sense that they are paying attention—that they sense and act with an interested concern that differs from but nevertheless echoes our own.

To take seriously the claim that systems such as these are capable of being agents in their own right is to begin to treat them as such. What this amounts to is a recognition of the potential of computational systems to possess and act on interests which may not align with those of the humans who created them. If what is at stake in the principle is not apparent in the given examples, it becomes clear when considering the military applications of artificial intelligence, algorithmic trading platforms, or that the duplicability of software often can lead to multiple version of a codebase existing on the network in minor variants, raising the possibility of the emergence of software populations subject to Darwinian forces. These considerations motivate an investigation into the presence of agency in novel nonhuman systems.

# Software Agents

A central problem in the development of software agents was to provide a sufficiently clear definition of agency permissive enough to apply to some computer programs but restrictive enough to distinguish software agents from other programs. In a 1996 paper Stan Franklin and Art Graesser surveyed the various ways that agency had been defined by people working in the field, and offered the following synthesis:

An autonomous agent is a system situated within and a part of an environment that senses that environment and acts on it, over time, in pursuit of its own agenda and so as to effect what it senses in the future. [4]

This definition identifies five essential properties of agents. They must

* possess operational autonomy
* be embedded in an environment
* sense this environment
* act on this environment
* possess an agenda structuring the relationship between what it senses and how it acts.

At one end of the spectrum humans meet these criteria, being agents “with multiple, conflicting drives, multiple senses, multiple possible actions, and complex sophisticated control structures” [5]. At the other end are relatively simple systems, such as thermostats. [4]

That thermostats are considered agents by the lights of this definition highlights the fact that emphasis has been placed on behavioural dispositions, with mentalistic vocabulary such as ‘intentions’ and ‘subjectivity’ having been deliberately avoided. Definitions of this type are said to define weak agency. Definitions which do include the possession of mental states as a stipulation are said to define strong agency. [6]

The distinction between weak and strong agency is already partially blurred in the above definition. Words like ‘act’ and ‘agenda’ can themselves be interpreted in strong and weak ways. There is a sense of ‘act’ in which to say a system acts is just to say that it causally affects. This corresponds to the weak sense of action, yet there is also a stronger sense of action which is deeply linked with intentions. If someone flicks on a light switch to alert a burglar, this may be described as an act because of the presence of the intention to alert. If they had fallen on the switch and accidentally alerted the burglar this would not be considered an act in this stronger sense. [7]

Similarly, ‘agenda’ seems a dangerously mentalistic concept. Weak articulations can feel oddly circular, going something like this: a system possesses an agenda if there exists some outcome whose actualisation functions as an organising rationale for how its outputs (actions) relate systematically to its inputs (senses). If the thermostat is set to 21 degrees then keeping the house at this temperature allows us to predict that it will turn the heating on if the temperature drops to 18 degrees, or turn it off at 22. (The circularity here takes the form of an imperative: keep the house at 21 degrees, making the thermostat sound as if it’s bound by some ethereal moral law.)

When such an outcome exists it becomes tempting to describe it as a preferred outcome of the system. It may be said that the attribution of a preference is here functioning as no more than a convenient shorthand for the patterning of the blind mechanistic causality grinding away in the system’s innards. But even so, the question remains: why do mentalistic attributions in particular provide so convenient a shorthand?

This question highlights the fact that the distinction between strong and weak agency is sensitive to the wider considerations of intentions and purposes sketched in the introduction. The aim in what follows is to explore some concrete implementations of programs exhibiting each of the key properties of weak agents, alone and in combination. After that we will offer some experiments in how these weak agents may interact with each other, and ask whether this helps in any way to bridge the gap between weak and strong notions of agency in computational systems. Finally, we will tie our findings back to the general question of agency in a material world.

Experiments : One Process

1. Operational autonomy. Autonomy in this context refers to a program’s independence from moment-by-moment human control. Once it has been set running it waits, and responds to new input as and when it arises. For this reason we do not consider programs that compute an output from an input and then terminate, only those which run in an endless loop, terminating only when an exit command is received. 

2. Environmental embeddedness. The environment of a thing or process is the set of things or processes it can causally interact with, without which it would not be possible to sense and act at all.
	- Question: what is a program’s environment? 
	- The physical system it runs on:
	- a program which uses lots of CPU power and heats up the physical system *
	- Humans (I/O):
		+ a program provides aesthetically pleasing text to the user
		+ a program outputs funny text to the console
		+ a program generates aesthetically pleasing images
		+ a program gets started by a user
		+ a program get terminated by a user
		+ a program gets input from a user
	- Other programs and data structures
		+ a program starts another program
		+ a program terminates another program
		+ a program outputs text to a file
		+ a program reads text from a file
	- The wider world: human societies, physical process
		+ cryptographic ransom: a program mines bitcoin, which it has to pay to its user as rent. If it doesn’t meet its payment the user turns it off. It has to pay in pounds, placing its life in the hands of currency fluctuations resulting from unpredictable human activity.

3. Sensing. For a program to sense it must monitor it must get causal input from its environment in one way or another. 
	⁃ Obvious ways:
		+ a program reads system status from the OS *
	- Less obvious ways:
		+ fatal sensing: something in its environment (a human, another program, itself, a lightning strike) causes it to terminate. This is something of an edge case.
	
4. Acting. For a program to act it must affect something in its environment.
	- a program outputs text into a file *
	- a program removes text from a file *
	- a program kills another program directly
	- a program kills another program indirectly by changing something else in its environment which it depends on in some way (e.g. removing a text file it depends on and making it terminate) *
	
5. Sensing and acting with no agenda. 
	- a program reads system data and outputs random strings into a text file. There is no relationship between the program’s inputs and outputs *
	- a program reads system data and outputs it into a text file. There is a relationship (of identity) between input and output but this relationship has no future-directed organising rationale *
	
6. Sensing and acting with an agendas. 
	- software thermostat: a program runs so fast it heats up the CPU; but it also reads the CPU temperature from the system and slows down when it gets too hot. When it drops below a certain temperature it heats up again *
	- a program keeps a specific text file free from any instance of vowels. It continuously monitors the file for instances of a vowel and deletes it if it appears *
	- one program removes characters from a text file, but will crash if it gets to the end. another program monitors the size of the file and adds text if it gets too short. *

# Experiments : Multiple Interacting Processes

Several of the ‘single’ process experiments involve more than one program. But in these cases the causal interaction is one way—the actions of one affect the other, but not vice versa. True interaction requires causal lines running in both directions. For example:

- two programs run on two different devices. They each monitor the other’s system status, and take action to make sure the other program continues to run *
	
This system exhibits a two way causal interaction, yet it does not seem to cross any significant philosophical thresholds. The agency seems no less weak, and there is no sense that these programs act in a way it would be appropriate to describe as intentional. 

It was noted earlier that it is difficult to describe agendas without inviting attributions of preferences, and intentions to actualise these preferences. While the programs in our example have agendas (in the weak sense) these agendas have no relevance to the programs themselves—they do not need to ‘know about’ the agendas in any sense, neither the other’s nor their own. It seems that ‘agendas’ and ‘preferential outcomes’ are eluded to only for our benefit, helping us to understand (or in this case construct) what happens in the system.

Under what circumstances might ascriptions of preferences and intentions become significant not only for us as creators and observers, but for the programs themselves? Since the attribution of a preference to a system functions as a way to understand how it acts in relation to what it senses, the answer is, presumably: whenever a program needs to form a working prediction of the behaviour of another in the pursuit of its own agenda.

- a population of programs edit a single text file. Some act under an agenda for decreasing its size to zero, while others attempt to increase it indefinitely. Still others try to achieve a particular size and keep it there. In addition to operations on the text file each program can monitor the system and see what other processes are running, and has the capacity to terminate them. 

In this example it is in a program’s interest to make a judgement about the agenda of another. It will further the agenda of a text-increasing program to terminate a text-decreasing program, and vice versa. Whether the agenda of a text-maintaining program will be furthered by terminating a program of a given agenda will depend on the current size of the text file. 

Of course, to make such judgements programs would need to be equipped with ways of detecting agendas: sensual apparatus to measure behaviours and quasi-cognitive capacities for drawing inferences. But in such a system, one which includes not only interests but conflicts of interest, we begin to glimpse a narrative of how agents might come to regard each other as such, how they might begin to act towards each other as beings with preferences and goals.

# Conclusions

We have constructed experiments—real and imagined—which implement in simple computer programs the characteristics of weak agency. This has prompted a consideration of the environment of computational process, revealing some wider entanglements with the human and nonhuman world through which software may affect and be affected in more obscure ways. 

In additional to this we have considered interactions in systems of multiple weak agents with differing agendas, in which we (perhaps) glimpse the stirrings of stronger forms of agency. Determinists may argue that no such thing has occurred. That attributing an intention to an agent may usefully serve as a predictor of its future behaviour is just to say that it is a good heuristic, getting it right most of the time when its complexity makes the deterministic calculation which would get it right all of the time unfeasible. 

But still, if this is so then the same may be said of the intentions (and other mental states) we attribute to each other and ourselves. Where would that leave us? Are we faced with a world where intentions exist only as fictions or social constructs? Or does this contention depend on making unfair demands on what it would take to concede that intentions are real, demands which stem from deeper anthropomorphisms in the conception of the relationships between language, truth and the world?

# References

1. Karen Barad, Posthuman Performativity: Toward an Understanding of How Matter Comes to Matter
2. Ulrik Ekman, Complex Ubiquity-Effects
4. Stan Franklin and Art Graesser, Is it an Agent, or just a Program? A Taxonomy for Autonomous Agents.
5. Stan Franklin, Artificial Minds.
6. Michael Woolridge online CS course: http://www.cs.ox.ac.uk/people/michael.wooldridge/pubs/ker95/subsubsectionstar3_1_1_1.html
7. Robert Brandom, Articulating Reasons: An Introduction to Inferentialism 
8. Karan Barad, Agential Realism.
